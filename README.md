# Triangular_solver

Решение задачи "СЛАУ с треугольными матрицами" для курса **Параллельное программирование**.

---

## Основная задача

Решается система линейных уравнений вида:

- **Нижнетреугольная матрица** \(Lx = b\):

\[
x_1 = \frac{b_1}{L_{11}}, \quad
x_i = \frac{b_i - \sum_{j=1}^{i-1} L_{ij} x_j}{L_{ii}}, \quad i=2..n
\]

- **Верхнетреугольная матрица** \(Ux = b\):

\[
x_n = \frac{b_n}{U_{nn}}, \quad
x_i = \frac{b_i - \sum_{j=i+1}^{n} U_{ij} x_j}{U_{ii}}, \quad i=n-1..1
\]

> Важно: эти алгоритмы последовательны по определению — каждый элемент зависит от предыдущих (для нижней) или последующих (для верхней) строк. Это накладывает ограничения на параллельность.

---

## Реализованные подходы к распараллеливанию

### 1. C + pthreads
- Распараллеливаем **вычисление суммы**:
\[
\sum L_{ij} x_j \quad или \quad \sum U_{ij} x_j
\]

- Элементы `x[i]` вычисляются последовательно, но сумма внутри строки вычисляется потоками.
- Используется `pthread_barrier_t` для синхронизации при необходимости.

Пример логики:

```c
for (i = 0; i < n; i++) {
    parallel_sum(i); // потоки параллельно считают сумму
    x[i] = (b[i] - sum) / L[i][i];
}
```
### 2. C + MPI
- Матрица и вектор b распределяются между процессами.
- Каждый процесс вычисляет часть суммы, потом делается MPI_Reduce или MPI_Bcast для x[i].
Эффективно для больших матриц на кластере.

### 3. Python + MPI (mpi4py)
- Логика аналогична C+MPI.
- Используем numpy для матриц и векторов.
Распараллеливание: comm.Allreduce() для суммы и comm.Bcast() для передачи x[i] всем процессам.

### 4. C + OpenMP
- Используется директива #pragma omp parallel for reduction(+:sum) для суммирования элементов строки.
- Легко вставляется в последовательный код.
- Ограничение ускорения — зависимость между элементами x[i].
Пример:
```
for (i = 0; i < n; i++) {
    double sum = 0;
    #pragma omp parallel for reduction(+:sum)
    for (j = 0; j < i; j++)
        sum += L[i][j] * x[j];
    x[i] = (b[i] - sum) / L[i][i];
}
```
---

## Локальное тестирование
1. Набор тестов
- Малые матрицы: 2x2, 3x3, 5x5 — ручная проверка.
- Средние матрицы: 10x10, 50x50 — проверка через numpy.linalg.solve.
- Большие матрицы: 100x100 — проверка масштабируемости.
Генерация треугольных матриц:
L = np.tril(np.random.rand(n,n) + 1)  # нижняя
U = np.triu(np.random.rand(n,n) + 1)  # верхняя
b = np.random.rand(n)

2. Проверка корректности
- Вычисляется решение `x_alg` с помощью выбранного алгоритма.
- Проверяется соответствие эталонному решению, вычисленному с помощью `numpy.linalg.solve` или аналогичного метода.
- Критерий проверки:

\[
\|Ax - b\|_\infty < \text{tol}, \quad \text{tol} = 1\text{e-12..1e-9}
\]

- Для автоматизации использован скрипт `run_all_test.py`, который последовательно запускает все реализации на разных размерах матриц и сравнивает результаты с эталоном.

---

## 3. Запуск на суперкомпьютере

На суперкомпьютере решения треугольных СЛАУ запускались с разным числом потоков и процессов с помощью скрипта ```run_scaling_hpc.sh```. От 1 до 48 для потоков и от 1 до 112 для MPI процессов.
Запускались тестовые файлы ```test_A.txt``` и ```test_b.txt```.
Для проверки корректности результатов и их записи в csv файл для дальнейщего анализа был использован скрипт ```check_and_save_csv.py```.

[Смотреть запись запуска на суперкомпьютере](https://drive.google.com/file/d/1aOrPhIOkHyv2BpxNTkTw11_YFWC3w18w/view?usp=sharing)

По результатам запусков был построен график результаты работы 4 реализаций относительно количества потоков/процессов.

![Скорость выполнения скриптов](result.png)



