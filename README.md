# Triangular_solver

Решение задачи "СЛАУ с треугольными матрицами" для курса **Параллельное программирование**.

---

## Основная задача

Решается система линейных уравнений вида:

- **Нижнетреугольная матрица** \(Lx = b\):

x1 = b1 / L11  
xi = (bi - sum_{j=1}^{i-1} Lij * xj) / Lii,  для i = 2..n

- **Верхнетреугольная матрица** \(Ux = b\):

xn = bn / Unn  
xi = (bi - sum_{j=i+1}^{n} Uij * xj) / Uii,  для i = n-1..1

> Важно: эти алгоритмы последовательны по определению — каждый элемент зависит от предыдущих (для нижней) или последующих (для верхней) строк. Это накладывает ограничения на параллельность.

---

## Реализованные подходы к распараллеливанию

### 1. C + pthreads
- Распараллеливаем **вычисление суммы**:
- 
sum = Σ Lij * xj   (для нижнетреугольной матрицы)  
sum = Σ Uij * xj   (для верхнетреугольной матрицы)

- Элементы `x[i]` вычисляются последовательно, но сумма внутри строки вычисляется потоками.
- Используется `pthread_barrier_t` для синхронизации при необходимости.

Пример логики:

```c
for (i = 0; i < n; i++) {
    parallel_sum(i); // потоки параллельно считают сумму
    x[i] = (b[i] - sum) / L[i][i];
}
```
### 2. C + MPI
- Матрица и вектор b распределяются между процессами.
- Каждый процесс вычисляет часть суммы, потом делается MPI_Reduce или MPI_Bcast для x[i].
Эффективно для больших матриц на кластере.

### 3. Python + MPI (mpi4py)
- Логика аналогична C+MPI.
- Используем numpy для матриц и векторов.
Распараллеливание: comm.Allreduce() для суммы и comm.Bcast() для передачи x[i] всем процессам.

### 4. C + OpenMP
- Используется директива #pragma omp parallel for reduction(+:sum) для суммирования элементов строки.
- Легко вставляется в последовательный код.
- Ограничение ускорения — зависимость между элементами x[i].
Пример:
```
for (i = 0; i < n; i++) {
    double sum = 0;
    #pragma omp parallel for reduction(+:sum)
    for (j = 0; j < i; j++)
        sum += L[i][j] * x[j];
    x[i] = (b[i] - sum) / L[i][i];
}
```
---

## Локальное тестирование
1. Набор тестов
- Малые матрицы: 2x2, 3x3, 5x5 — ручная проверка.
- Средние матрицы: 10x10, 50x50 — проверка через numpy.linalg.solve.
- Большие матрицы: 100x100 — проверка масштабируемости.
Генерация треугольных матриц:
L = np.tril(np.random.rand(n,n) + 1)  # нижняя
U = np.triu(np.random.rand(n,n) + 1)  # верхняя
b = np.random.rand(n)

2. Проверка корректности
- Вычисляется решение `x_alg` с помощью выбранного алгоритма.
- Проверяется соответствие эталонному решению, вычисленному с помощью `numpy.linalg.solve` или аналогичного метода.
- Критерий проверки:

||Ax - b||_∞ < tol,  где tol = 1e-12..1e-9

- Для автоматизации использован скрипт `run_all_test.py`, который последовательно запускает все реализации на разных размерах матриц и сравнивает результаты с эталоном.

---

## 3. Запуск на суперкомпьютере

На суперкомпьютере решения треугольных СЛАУ запускались с разным числом потоков и процессов с помощью скрипта ```run_scaling_hpc.sh```. От 1 до 48 для потоков и от 1 до 112 для MPI процессов.
Запускались тестовые файлы ```test_A.txt``` и ```test_b.txt```, размер матрицы 10000 на 10000.
Для проверки корректности результатов и их записи в csv файл для дальнейщего анализа был использован скрипт ```check_and_save_csv.py```.

[Смотреть запись запуска на суперкомпьютере](https://drive.google.com/file/d/1aOrPhIOkHyv2BpxNTkTw11_YFWC3w18w/view?usp=sharing)

По результатам запусков был построен график результаты работы 4 реализаций относительно количества потоков/процессов.

### График ускорения (Speedup) для параллельных реализаций

Для оценки эффективности распараллеливания построен график ускорения:

Speedup = T1 / Tp

- \(T_1\) — время выполнения алгоритма на 1 потоке или процессе.  
- \(T_p\) — время выполнения на \(p\) потоках или процессах.  
 
#### Интерпретация 
- Для треугольных СЛАУ ускорение ограничено зависимостью по элементам вектора решения, так как каждый элемент зависит от предыдущих (нижняя) или следующих (верхняя) элементов.  



![Скорость выполнения скриптов](result.png)



